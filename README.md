# **Earthquake Data Pipeline - Databricks & Azure Data Lake** 

## ğŸ“Œ VisÃ£o Geral

Este repositÃ³rio contÃ©m uma **pipeline de dados distribuÃ­da e escalÃ¡vel** construÃ­da no **Azure Databricks**, projetada para coletar, transformar e organizar dados sÃ­smicos da **USGS Earthquake API**.

A soluÃ§Ã£o segue o modelo **Medallion Architecture** (**Bronze, Silver e Gold**), permitindo ingestÃ£o eficiente, tratamento de dados e armazenamento estruturado para anÃ¡lise avanÃ§ada.

O pipeline Ã© **totalmente orquestrado dentro do Databricks**, sem necessidade de ferramentas externas como Data Factory ou Synapse Analytics.

---

## ğŸ  Arquitetura da Pipeline

A execuÃ§Ã£o do fluxo de dados Ã© gerenciada por um **workflow no Databricks**, que segue a seguinte sequÃªncia:

### ğŸ”¹ **Origem dos Dados (Azure External Locations)**

- Os dados sÃ£o extraÃ­dos a partir da funcionalidade **Dados Externos** no Databricks.
- Diferente da abordagem tradicional com **Azure Data Factory**, utilizamos **credenciais gerenciadas** no prÃ³prio Databricks.
- A credencial foi criada e configurada para acessar **containers especÃ­ficos no Azure Data Lake**, garantindo controle granular de permissÃµes.
- As localizaÃ§Ãµes externas foram configuradas para as camadas **Bronze, Silver e Gold**, permitindo que os dados sejam acessados diretamente no Databricks sem necessidade de cÃ³pias adicionais.

### ğŸ”¹ **Controle de Acesso via IAM**

- O acesso ao **Unity Catalog** foi configurado via **IAM (Identity and Access Management)** no Azure.
- A identidade gerenciada `unity-catalog-access-connector` foi adicionada como **Colaboradora de Dados do Storage Blob**, garantindo que os notebooks e workflows no Databricks possam acessar os containers do Data Lake de forma segura.
- Esse mÃ©todo reduz a necessidade de armazenar credenciais dentro do cÃ³digo e melhora a governanÃ§a dos dados.

### ğŸ”¸ **Bronze Layer (IngestÃ£o de Dados Brutos)**

- **ConexÃ£o com a API**: A API da **USGS Earthquake** Ã© acessada utilizando o mÃ©todo `GET` do mÃ³dulo `requests`, permitindo a obtenÃ§Ã£o de dados sÃ­smicos em tempo real.
- **ParÃ¢metros Configurados**: A data inicial e final sÃ£o definidas dinamicamente e passadas como query parameters.
- **Leitura e ConversÃ£o de Dados**: O retorno da API (JSON) Ã© convertido para um **DataFrame PySpark** para manipulaÃ§Ã£o eficiente.
- **Armazenamento no Azure Data Lake**: Os dados sÃ£o salvos no formato **Parquet**, garantindo eficiÃªncia e compatibilidade com as prÃ³ximas camadas.
- **DefiniÃ§Ã£o de VariÃ¡veis para PrÃ³ximas Etapas**:
  ```python
  start_date = "2025-02-10"
  bronze_adls = "abfss://bronze@yourdatalake.dfs.core.windows.net/earthquake_data"
  silver_adls = "abfss://silver@yourdatalake.dfs.core.windows.net/earthquake_data"
  dbutils.jobs.taskValues.set(
      key="bronze_output",
      value={"start_date": start_date, "bronze_adls": bronze_adls, "silver_adls": silver_adls}
  )
  ```

### ğŸ”¸ **Silver Layer (TransformaÃ§Ã£o e Enriquecimento)**

- **NormalizaÃ§Ã£o dos dados**, aplicando ajustes estruturais e padronizaÃ§Ãµes.
- **Enriquecimento das informaÃ§Ãµes**, incluindo junÃ§Ãµes e manipulaÃ§Ãµes adicionais.
- **CriaÃ§Ã£o de colunas estruturadas** para facilitar anÃ¡lises futuras.
- **Escrita dos dados na camada Silver** dentro do **Azure Data Lake**.

### ğŸ† **Gold Layer (Dados Prontos para Consumo)**

- AgregaÃ§Ã£o de dados e cÃ¡lculos estatÃ­sticos relevantes.
- ConversÃ£o dos dados para um formato otimizado para consultas.
- ExportaÃ§Ã£o dos dados refinados para a camada **Gold** no **Azure Data Lake**.

---

## âš™ï¸ Tecnologias Utilizadas

âœ… **Azure Databricks** â†’ OrquestraÃ§Ã£o e processamento distribuÃ­do com **Apache Spark**  
âœ… **Azure Data Lake Storage (ADLS)** â†’ Armazenamento estruturado em camadas  
âœ… **Unity Catalog** â†’ GovernanÃ§a e controle de acesso centralizado  
âœ… **Parquet Format** â†’ Arquivos otimizados para leitura e performance  
âœ… **PySpark** â†’ ManipulaÃ§Ã£o eficiente de grandes volumes de dados

---

## ğŸ”„ Fluxo de ExecuÃ§Ã£o

A pipeline Ã© executada via **workflow no Databricks**, conforme o JSON do job:

1ï¸âƒ£ **Bronze** â†’ Coleta e armazena os dados brutos da API.  
2ï¸âƒ£ **Silver** â†’ Limpeza, transformaÃ§Ã£o e normalizaÃ§Ã£o dos dados.  
3ï¸âƒ£ **Gold** â†’ AgregaÃ§Ã£o e otimizaÃ§Ã£o dos dados para anÃ¡lise.

Cada etapa Ã© executada apenas **se a anterior for bem-sucedida** (`run_if: ALL_SUCCESS`), garantindo a integridade do pipeline.

---

## ğŸš€ BenefÃ­cios da SoluÃ§Ã£o

âœ”ï¸ **AutomatizaÃ§Ã£o Completa** â†’ Fluxo 100% gerenciado dentro do Databricks.  
âœ”ï¸ **Escalabilidade** â†’ Capacidade de processar grandes volumes de dados sÃ­smicos.  
âœ”ï¸ **EficiÃªncia e OrganizaÃ§Ã£o** â†’ Dados estruturados em camadas para fÃ¡cil consumo.  
âœ”ï¸ **Alto Desempenho** â†’ Uso otimizado do Apache Spark para processamento distribuÃ­do.  
âœ”ï¸ **SeguranÃ§a Aprimorada** â†’ Uso de IAM e Unity Catalog para controle de acesso seguro.

---

## ğŸ’¡ ContribuiÃ§Ãµes

Sinta-se Ã  vontade para sugerir melhorias, abrir issues ou contribuir para otimizar a pipeline!

ğŸ“© Para dÃºvidas ou sugestÃµes, entre em contato.


